{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering Analysis - Mall Customer Segmentation\n",
    "\n",
    "This notebook explores and compares multiple clustering algorithms on the Mall Customer Segmentation dataset.\n",
    "\n",
    "**Clustering Models:**\n",
    "- K-Means (baseline)\n",
    "- DBSCAN\n",
    "- K-Medoids\n",
    "- Agglomerative Clustering\n",
    "- Gaussian Mixture Models\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Exploratory Data Analysis and Data Preprocessing\n",
    "\n",
    "This section performs comprehensive exploratory data analysis and prepares the data for modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 1.1: Exploratory Data Analysis\n",
    "\n",
    "In this section, we'll:\n",
    "- Load and inspect the dataset\n",
    "- Perform statistical summaries\n",
    "- Analyze correlations between features\n",
    "- Identify outliers and missing values\n",
    "- Visualize distributions and relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set visualization style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "# Create output directory for visualizations\n",
    "viz_dir = Path('visualizations')\n",
    "viz_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"vjchoudhary7/customer-segmentation-tutorial-in-python\")\n",
    "print(f\"Dataset path: {path}\")\n",
    "\n",
    "# Load the CSV file\n",
    "data_path = Path(path) / 'Mall_Customers.csv'\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "print(f\"\\nDataset loaded successfully!\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic Dataset Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display dataset info\n",
    "print(\"Dataset Information:\")\n",
    "print(\"=\" * 50)\n",
    "df.info()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"\\nColumn Names:\")\n",
    "print(df.columns.tolist())\n",
    "\n",
    "print(\"\\nDataset Shape:\")\n",
    "print(f\"Rows: {df.shape[0]}\")\n",
    "print(f\"Columns: {df.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Statistical Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary of numerical features\n",
    "print(\"Statistical Summary of Numerical Features:\")\n",
    "print(\"=\" * 50)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary of categorical features\n",
    "print(\"\\nStatistical Summary of Categorical Features:\")\n",
    "print(\"=\" * 50)\n",
    "df.describe(include=['object'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Missing Value Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing Value Analysis:\")\n",
    "print(\"=\" * 50)\n",
    "missing_values = df.isnull().sum()\n",
    "missing_percentage = (df.isnull().sum() / len(df)) * 100\n",
    "\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing Count': missing_values,\n",
    "    'Missing Percentage': missing_percentage\n",
    "})\n",
    "\n",
    "print(missing_df)\n",
    "print(f\"\\nTotal missing values: {df.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize missing values\n",
    "if df.isnull().sum().sum() > 0:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.heatmap(df.isnull(), cbar=True, cmap='viridis', yticklabels=False)\n",
    "    plt.title('Missing Values Heatmap', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(viz_dir / 'missing_values_heatmap.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No missing values detected in the dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categorical Feature Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze categorical features\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "print(\"Categorical Features Analysis:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for col in categorical_cols:\n",
    "    print(f\"\\n{col}:\")\n",
    "    print(f\"  Unique values: {df[col].nunique()}\")\n",
    "    print(f\"  Value counts:\")\n",
    "    print(df[col].value_counts())\n",
    "    print(f\"\\n  Percentage distribution:\")\n",
    "    print(df[col].value_counts(normalize=True) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize categorical features\n",
    "if len(categorical_cols) > 0:\n",
    "    n_cols = len(categorical_cols)\n",
    "    fig, axes = plt.subplots(1, n_cols, figsize=(6*n_cols, 5))\n",
    "    \n",
    "    if n_cols == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for idx, col in enumerate(categorical_cols):\n",
    "        df[col].value_counts().plot(kind='bar', ax=axes[idx], color='steelblue')\n",
    "        axes[idx].set_title(f'Distribution of {col}', fontsize=12, fontweight='bold')\n",
    "        axes[idx].set_xlabel(col, fontsize=10)\n",
    "        axes[idx].set_ylabel('Count', fontsize=10)\n",
    "        axes[idx].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(viz_dir / 'categorical_features_distribution.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distribution Analysis of Numerical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get numerical columns\n",
    "numerical_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "print(\"Numerical Features:\")\n",
    "print(numerical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution plots for numerical features\n",
    "n_cols = len(numerical_cols)\n",
    "n_rows = (n_cols + 1) // 2\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, 2, figsize=(14, 5*n_rows))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, col in enumerate(numerical_cols):\n",
    "    axes[idx].hist(df[col], bins=30, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "    axes[idx].set_title(f'Distribution of {col}', fontsize=12, fontweight='bold')\n",
    "    axes[idx].set_xlabel(col, fontsize=10)\n",
    "    axes[idx].set_ylabel('Frequency', fontsize=10)\n",
    "    axes[idx].axvline(df[col].mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {df[col].mean():.2f}')\n",
    "    axes[idx].axvline(df[col].median(), color='green', linestyle='--', linewidth=2, label=f'Median: {df[col].median():.2f}')\n",
    "    axes[idx].legend()\n",
    "    axes[idx].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Hide extra subplots\n",
    "for idx in range(len(numerical_cols), len(axes)):\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(viz_dir / 'numerical_features_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plots for numerical features to identify outliers\n",
    "fig, axes = plt.subplots(n_rows, 2, figsize=(14, 5*n_rows))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, col in enumerate(numerical_cols):\n",
    "    axes[idx].boxplot(df[col].dropna(), vert=True, patch_artist=True,\n",
    "                      boxprops=dict(facecolor='lightblue', color='blue'),\n",
    "                      whiskerprops=dict(color='blue'),\n",
    "                      capprops=dict(color='blue'),\n",
    "                      medianprops=dict(color='red', linewidth=2))\n",
    "    axes[idx].set_title(f'Box Plot of {col}', fontsize=12, fontweight='bold')\n",
    "    axes[idx].set_ylabel(col, fontsize=10)\n",
    "    axes[idx].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Hide extra subplots\n",
    "for idx in range(len(numerical_cols), len(axes)):\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(viz_dir / 'numerical_features_boxplot.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outlier Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify outliers using IQR method\n",
    "def identify_outliers_iqr(dataframe, column):\n",
    "    Q1 = dataframe[column].quantile(0.25)\n",
    "    Q3 = dataframe[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    outliers = dataframe[(dataframe[column] < lower_bound) | (dataframe[column] > upper_bound)]\n",
    "    return outliers, lower_bound, upper_bound\n",
    "\n",
    "print(\"Outlier Analysis (IQR Method):\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for col in numerical_cols:\n",
    "    outliers, lower, upper = identify_outliers_iqr(df, col)\n",
    "    print(f\"\\n{col}:\")\n",
    "    print(f\"  Lower Bound: {lower:.2f}\")\n",
    "    print(f\"  Upper Bound: {upper:.2f}\")\n",
    "    print(f\"  Number of outliers: {len(outliers)} ({(len(outliers)/len(df)*100):.2f}%)\")\n",
    "    if len(outliers) > 0:\n",
    "        print(f\"  Outlier values range: [{outliers[col].min():.2f}, {outliers[col].max():.2f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute correlation matrix\n",
    "correlation_matrix = df[numerical_cols].corr()\n",
    "\n",
    "print(\"Correlation Matrix:\")\n",
    "print(\"=\" * 50)\n",
    "print(correlation_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize correlation heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm', \n",
    "            square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\n",
    "plt.title('Correlation Matrix Heatmap', fontsize=14, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.savefig(viz_dir / 'correlation_heatmap.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find highly correlated feature pairs\n",
    "def get_high_correlations(corr_matrix, threshold=0.7):\n",
    "    high_corr_pairs = []\n",
    "    for i in range(len(corr_matrix.columns)):\n",
    "        for j in range(i+1, len(corr_matrix.columns)):\n",
    "            if abs(corr_matrix.iloc[i, j]) > threshold:\n",
    "                high_corr_pairs.append((\n",
    "                    corr_matrix.columns[i],\n",
    "                    corr_matrix.columns[j],\n",
    "                    corr_matrix.iloc[i, j]\n",
    "                ))\n",
    "    return high_corr_pairs\n",
    "\n",
    "high_corr = get_high_correlations(correlation_matrix, threshold=0.7)\n",
    "\n",
    "print(\"\\nHighly Correlated Feature Pairs (|r| > 0.7):\")\n",
    "print(\"=\" * 50)\n",
    "if high_corr:\n",
    "    for feat1, feat2, corr_val in high_corr:\n",
    "        print(f\"{feat1} <-> {feat2}: {corr_val:.3f}\")\n",
    "else:\n",
    "    print(\"No highly correlated feature pairs found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pairwise Scatter Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pairwise scatter plot for numerical features\n",
    "# Exclude ID column if present for better visualization\n",
    "feature_cols = [col for col in numerical_cols if 'id' not in col.lower()]\n",
    "\n",
    "if len(feature_cols) > 1:\n",
    "    pairplot = sns.pairplot(df[feature_cols], diag_kind='hist', \n",
    "                            plot_kws={'alpha': 0.6, 's': 30, 'edgecolor': 'k'},\n",
    "                            diag_kws={'edgecolor': 'k', 'alpha': 0.7})\n",
    "    pairplot.fig.suptitle('Pairwise Scatter Plots of Numerical Features', \n",
    "                          fontsize=16, fontweight='bold', y=1.01)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(viz_dir / 'pairwise_scatter_plots.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Not enough numerical features for pairwise scatter plots.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary Statistics by Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze numerical features grouped by categorical features\n",
    "if len(categorical_cols) > 0 and len(feature_cols) > 0:\n",
    "    for cat_col in categorical_cols:\n",
    "        print(f\"\\nSummary Statistics by {cat_col}:\")\n",
    "        print(\"=\" * 70)\n",
    "        print(df.groupby(cat_col)[feature_cols].describe().round(2))\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize numerical features by categorical features\n",
    "if len(categorical_cols) > 0 and len(feature_cols) > 0:\n",
    "    for cat_col in categorical_cols:\n",
    "        n_features = len(feature_cols)\n",
    "        fig, axes = plt.subplots(1, n_features, figsize=(6*n_features, 5))\n",
    "        \n",
    "        if n_features == 1:\n",
    "            axes = [axes]\n",
    "        \n",
    "        for idx, num_col in enumerate(feature_cols):\n",
    "            df.boxplot(column=num_col, by=cat_col, ax=axes[idx], patch_artist=True)\n",
    "            axes[idx].set_title(f'{num_col} by {cat_col}', fontsize=12, fontweight='bold')\n",
    "            axes[idx].set_xlabel(cat_col, fontsize=10)\n",
    "            axes[idx].set_ylabel(num_col, fontsize=10)\n",
    "        \n",
    "        plt.suptitle('')  # Remove the automatic title\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(viz_dir / f'numerical_by_{cat_col}.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Key Insights from EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of key findings\n",
    "print(\"KEY INSIGHTS FROM EXPLORATORY DATA ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\n1. Dataset Overview:\")\n",
    "print(f\"   - Total samples: {df.shape[0]}\")\n",
    "print(f\"   - Total features: {df.shape[1]}\")\n",
    "print(f\"   - Numerical features: {len(numerical_cols)}\")\n",
    "print(f\"   - Categorical features: {len(categorical_cols)}\")\n",
    "\n",
    "print(f\"\\n2. Data Quality:\")\n",
    "print(f\"   - Missing values: {df.isnull().sum().sum()}\")\n",
    "print(f\"   - Duplicate rows: {df.duplicated().sum()}\")\n",
    "\n",
    "print(f\"\\n3. Outliers Detected:\")\n",
    "for col in numerical_cols:\n",
    "    outliers, _, _ = identify_outliers_iqr(df, col)\n",
    "    if len(outliers) > 0:\n",
    "        print(f\"   - {col}: {len(outliers)} outliers ({(len(outliers)/len(df)*100):.2f}%)\")\n",
    "\n",
    "print(f\"\\n4. Feature Correlations:\")\n",
    "high_corr = get_high_correlations(correlation_matrix, threshold=0.7)\n",
    "if high_corr:\n",
    "    for feat1, feat2, corr_val in high_corr:\n",
    "        print(f\"   - {feat1} and {feat2}: {corr_val:.3f}\")\n",
    "else:\n",
    "    print(\"   - No strong correlations (|r| > 0.7) detected\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### Section 1.2: Data Preprocessing Pipeline\n\nIn this section, we'll:\n- Select the features for clustering (Annual Income and Spending Score)\n- Handle any missing values\n- Scale/normalize the features\n- Create two datasets: one with preprocessing and one without\n- Visualize the selected features and their relationship",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Import preprocessing libraries\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n\nprint(\"Preprocessing libraries imported successfully!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "#### Feature Selection\n\nWe'll focus on two key features for clustering:\n- **Annual Income (k$)**: Customer's annual income\n- **Spending Score (1-100)**: Score assigned by the mall based on customer behavior and spending nature",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Select the features for clustering\nselected_features = ['Annual Income (k$)', 'Spending Score (1-100)']\n\n# Check if these columns exist in the dataframe\nprint(\"Checking for selected features in dataset...\")\nprint(\"=\" * 50)\nfor feature in selected_features:\n    if feature in df.columns:\n        print(f\"✓ {feature} found\")\n    else:\n        print(f\"✗ {feature} NOT found\")\n        print(f\"Available columns: {df.columns.tolist()}\")\n\nprint(f\"\\nSelected features: {selected_features}\")\nprint(f\"\\nShape after feature selection: {df[selected_features].shape}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Create the clustering dataset with selected features\nX = df[selected_features].copy()\n\nprint(\"Clustering Dataset Created:\")\nprint(\"=\" * 50)\nprint(f\"Shape: {X.shape}\")\nprint(f\"\\nFirst few rows:\")\nprint(X.head(10))\nprint(f\"\\nStatistical Summary:\")\nprint(X.describe())",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "#### Check for Missing Values in Selected Features",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Check for missing values in selected features\nprint(\"Missing Values in Selected Features:\")\nprint(\"=\" * 50)\nmissing_in_X = X.isnull().sum()\nprint(missing_in_X)\nprint(f\"\\nTotal missing values: {X.isnull().sum().sum()}\")\n\nif X.isnull().sum().sum() > 0:\n    print(\"\\nHandling missing values...\")\n    # For this dataset, we'll drop rows with missing values\n    X_clean = X.dropna()\n    print(f\"Rows before: {len(X)}\")\n    print(f\"Rows after: {len(X_clean)}\")\n    print(f\"Rows removed: {len(X) - len(X_clean)}\")\n    X = X_clean\nelse:\n    print(\"\\nNo missing values found. Proceeding with full dataset.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "#### Visualize Selected Features Before Preprocessing",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Scatter plot of the two selected features\nplt.figure(figsize=(10, 6))\nplt.scatter(X[selected_features[0]], X[selected_features[1]], \n            alpha=0.6, s=50, edgecolor='k', linewidth=0.5)\nplt.xlabel(selected_features[0], fontsize=12, fontweight='bold')\nplt.ylabel(selected_features[1], fontsize=12, fontweight='bold')\nplt.title('Scatter Plot: Annual Income vs Spending Score (Before Preprocessing)', \n          fontsize=14, fontweight='bold')\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.savefig(viz_dir / 'selected_features_scatter_before_preprocessing.png', dpi=300, bbox_inches='tight')\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Distribution plots for selected features\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\nfor idx, feature in enumerate(selected_features):\n    axes[idx].hist(X[feature], bins=30, color='steelblue', edgecolor='black', alpha=0.7)\n    axes[idx].set_title(f'Distribution of {feature}', fontsize=12, fontweight='bold')\n    axes[idx].set_xlabel(feature, fontsize=10)\n    axes[idx].set_ylabel('Frequency', fontsize=10)\n    axes[idx].axvline(X[feature].mean(), color='red', linestyle='--', linewidth=2, \n                      label=f'Mean: {X[feature].mean():.2f}')\n    axes[idx].axvline(X[feature].median(), color='green', linestyle='--', linewidth=2, \n                      label=f'Median: {X[feature].median():.2f}')\n    axes[idx].legend()\n    axes[idx].grid(axis='y', alpha=0.3)\n\nplt.tight_layout()\nplt.savefig(viz_dir / 'selected_features_distribution_before_preprocessing.png', dpi=300, bbox_inches='tight')\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "#### Create Two Datasets: With and Without Preprocessing",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Dataset 1: Without Preprocessing (Original Data)\nX_original = X.copy()\n\nprint(\"Dataset 1: Without Preprocessing\")\nprint(\"=\" * 50)\nprint(f\"Shape: {X_original.shape}\")\nprint(f\"\\nStatistics:\")\nprint(X_original.describe())\n\n# Save for later use\nprint(\"\\nDataset saved as 'X_original' for clustering without preprocessing.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Dataset 2: With Preprocessing (StandardScaler)\n# Initialize the scaler\nscaler = StandardScaler()\n\n# Fit and transform the data\nX_scaled = scaler.fit_transform(X)\n\n# Convert back to DataFrame for easier handling\nX_scaled = pd.DataFrame(X_scaled, columns=selected_features, index=X.index)\n\nprint(\"Dataset 2: With Preprocessing (StandardScaler)\")\nprint(\"=\" * 50)\nprint(f\"Shape: {X_scaled.shape}\")\nprint(f\"\\nScaler parameters:\")\nprint(f\"  Mean: {scaler.mean_}\")\nprint(f\"  Standard Deviation: {np.sqrt(scaler.var_)}\")\nprint(f\"\\nStatistics after scaling:\")\nprint(X_scaled.describe())\n\n# Save for later use\nprint(\"\\nDataset saved as 'X_scaled' for clustering with preprocessing.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "#### Visualize Scaled Features",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Scatter plot comparison: Before and After Scaling\nfig, axes = plt.subplots(1, 2, figsize=(16, 6))\n\n# Before scaling\naxes[0].scatter(X_original[selected_features[0]], X_original[selected_features[1]], \n                alpha=0.6, s=50, edgecolor='k', linewidth=0.5, color='steelblue')\naxes[0].set_xlabel(selected_features[0], fontsize=11, fontweight='bold')\naxes[0].set_ylabel(selected_features[1], fontsize=11, fontweight='bold')\naxes[0].set_title('Before Preprocessing (Original Scale)', fontsize=12, fontweight='bold')\naxes[0].grid(True, alpha=0.3)\n\n# After scaling\naxes[1].scatter(X_scaled[selected_features[0]], X_scaled[selected_features[1]], \n                alpha=0.6, s=50, edgecolor='k', linewidth=0.5, color='coral')\naxes[1].set_xlabel(f'{selected_features[0]} (Scaled)', fontsize=11, fontweight='bold')\naxes[1].set_ylabel(f'{selected_features[1]} (Scaled)', fontsize=11, fontweight='bold')\naxes[1].set_title('After Preprocessing (StandardScaler)', fontsize=12, fontweight='bold')\naxes[1].grid(True, alpha=0.3)\n\nplt.suptitle('Comparison: Original vs Scaled Features', fontsize=14, fontweight='bold', y=1.02)\nplt.tight_layout()\nplt.savefig(viz_dir / 'preprocessing_comparison_scatter.png', dpi=300, bbox_inches='tight')\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Distribution comparison: Before and After Scaling\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\n\nfor idx, feature in enumerate(selected_features):\n    # Before scaling\n    axes[idx, 0].hist(X_original[feature], bins=30, color='steelblue', edgecolor='black', alpha=0.7)\n    axes[idx, 0].set_title(f'{feature} - Original', fontsize=11, fontweight='bold')\n    axes[idx, 0].set_xlabel(feature, fontsize=10)\n    axes[idx, 0].set_ylabel('Frequency', fontsize=10)\n    axes[idx, 0].axvline(X_original[feature].mean(), color='red', linestyle='--', linewidth=2, \n                         label=f'Mean: {X_original[feature].mean():.2f}')\n    axes[idx, 0].legend()\n    axes[idx, 0].grid(axis='y', alpha=0.3)\n    \n    # After scaling\n    axes[idx, 1].hist(X_scaled[feature], bins=30, color='coral', edgecolor='black', alpha=0.7)\n    axes[idx, 1].set_title(f'{feature} - Scaled', fontsize=11, fontweight='bold')\n    axes[idx, 1].set_xlabel(f'{feature} (Scaled)', fontsize=10)\n    axes[idx, 1].set_ylabel('Frequency', fontsize=10)\n    axes[idx, 1].axvline(X_scaled[feature].mean(), color='red', linestyle='--', linewidth=2, \n                         label=f'Mean: {X_scaled[feature].mean():.2f}')\n    axes[idx, 1].legend()\n    axes[idx, 1].grid(axis='y', alpha=0.3)\n\nplt.suptitle('Distribution Comparison: Original vs Scaled Features', fontsize=14, fontweight='bold')\nplt.tight_layout()\nplt.savefig(viz_dir / 'preprocessing_comparison_distributions.png', dpi=300, bbox_inches='tight')\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "#### Data Preprocessing Summary",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Summary of preprocessing steps\nprint(\"DATA PREPROCESSING SUMMARY\")\nprint(\"=\" * 70)\nprint(f\"\\n1. Feature Selection:\")\nprint(f\"   - Selected features: {selected_features}\")\nprint(f\"   - Original dataset shape: {df.shape}\")\nprint(f\"   - Selected features shape: {X_original.shape}\")\n\nprint(f\"\\n2. Missing Values:\")\nprint(f\"   - Missing values in selected features: {X_original.isnull().sum().sum()}\")\nprint(f\"   - Action taken: {'Dropped rows' if X_original.isnull().sum().sum() > 0 else 'No action needed'}\")\n\nprint(f\"\\n3. Datasets Created:\")\nprint(f\"   - X_original (without preprocessing): {X_original.shape}\")\nprint(f\"     * Scale: Original units\")\nprint(f\"     * {selected_features[0]} range: [{X_original[selected_features[0]].min():.2f}, {X_original[selected_features[0]].max():.2f}]\")\nprint(f\"     * {selected_features[1]} range: [{X_original[selected_features[1]].min():.2f}, {X_original[selected_features[1]].max():.2f}]\")\n\nprint(f\"\\n   - X_scaled (with StandardScaler): {X_scaled.shape}\")\nprint(f\"     * Scale: Standardized (mean=0, std=1)\")\nprint(f\"     * {selected_features[0]} range: [{X_scaled[selected_features[0]].min():.2f}, {X_scaled[selected_features[0]].max():.2f}]\")\nprint(f\"     * {selected_features[1]} range: [{X_scaled[selected_features[1]].min():.2f}, {X_scaled[selected_features[1]].max():.2f}]\")\n\nprint(f\"\\n4. Ready for Clustering:\")\nprint(f\"   ✓ Two datasets prepared\")\nprint(f\"   ✓ No missing values\")\nprint(f\"   ✓ Features selected and scaled\")\nprint(f\"   ✓ Visualizations saved to '{viz_dir}' directory\")\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"Section 1.2 Complete! Ready for clustering analysis.\")\nprint(\"=\" * 70)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}